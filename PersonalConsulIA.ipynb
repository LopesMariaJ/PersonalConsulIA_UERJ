{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import argparse\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_community.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "#from get_embedding_function import get_embedding_function\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **argparse**: Facilitates the creation of command-line commands and allows the code to receive parameters directly via CLI.\n",
    "\n",
    "- **Chroma**: A vector database for storing and retrieving text embeddings. Utilizes the persistent directory `CHROMA_PATH`.\n",
    "\n",
    "- **ChatPromptTemplate**: Manages prompts for language models.\n",
    "\n",
    "- **Ollama**: Interface for using the Ollama language model, which responds to questions based on prompts.\n",
    "\n",
    "- **PyPDFDirectoryLoader**: Loads PDF documents from a directory.\n",
    "\n",
    "- **get_embedding_function**: Imports a custom function that creates text embeddings.\n",
    "\n",
    "- **RecursiveCharacterTextSplitter**: Splits documents into smaller chunks while maintaining overlap between them.\n",
    "\n",
    "- **Document**: Class used to organize and manipulate chunks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_PATH = 'chroma_directory'\n",
    "DATA_PATH = \"pdfs\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **CHROMA_PATH**: Directory where the Chroma database will be stored.\n",
    "\n",
    "- **DATA_PATH**: Directory from which PDFs will be loaded.\n",
    "\n",
    "- **PROMPT_TEMPLATE**: A prompt template used to request responses from the model, providing context and a question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    print(\"ETAPA 01 \" + str(document_loader))\n",
    "    \n",
    "    # Carrega os documentos do diretório\n",
    "    documents = document_loader.load()\n",
    "    \n",
    "    # Verifica e exibe quais arquivos foram encontrados\n",
    "    if not documents:\n",
    "        print(\"Nenhum arquivo PDF encontrado no diretório especificado.\")\n",
    "    else:\n",
    "        print(f\"ETAPA 02: {len(documents)} arquivos encontrados:\")\n",
    "        for doc in documents:\n",
    "            print(f\" - Arquivo: {doc.metadata['source']}\")\n",
    "    \n",
    "    # Divide os documentos em chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=800,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(\"ETAPA 03: \" + str(chunks))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Chame a função para verificar\n",
    "load_and_split_documents()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Objective**: Loads all PDF documents from the specified directory and splits them into chunks.\n",
    "\n",
    "- **document_loader**: Initializes the PDF loader.\n",
    "\n",
    "- **documents**: Loads all documents from the `DATA_PATH` directory.\n",
    "\n",
    "- **text_splitter**: Configures the splitter to divide documents into chunks of up to 2000 characters with an overlap of 800 characters.\n",
    "\n",
    "- **chunks**: Divided chunks are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_ids(chunks):\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Extrai as informações do chunk\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "\n",
    "        # Debug: Exibe o estado atual\n",
    "        print(f\"Iteração {i + 1}:\")\n",
    "        print(f\"  Fonte: {source}\")\n",
    "        print(f\"  Página: {page}\")\n",
    "        print(f\"  ID da Página Atual: {current_page_id}\")\n",
    "        print(f\"  Último ID da Página: {last_page_id}\")\n",
    "\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "\n",
    "        # Calcula o ID do chunk\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "\n",
    "        # Debug: Exibe o ID calculado e atribuído\n",
    "        print(f\"  ID do Chunk Calculado: {chunk_id}\")\n",
    "        print(f\"  Chunk Metadata Atualizada: {chunk.metadata}\")\n",
    "\n",
    "        # Atualiza o último ID da página\n",
    "        last_page_id = current_page_id\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Exemplo de chamada da função para verificação\n",
    "# Suponha que você tenha uma lista de chunks para passar para a função\n",
    "chunks = [\n",
    "    Document(metadata={\"source\": \"document1.pdf\", \"page\": 1}, page_content=\"Conteúdo da página 1\"),\n",
    "    Document(metadata={\"source\": \"document1.pdf\", \"page\": 1}, page_content=\"Conteúdo da página 1 - continuação\"),\n",
    "    Document(metadata={\"source\": \"document1.pdf\", \"page\": 2}, page_content=\"Conteúdo da página 2\"),\n",
    "    Document(metadata={\"source\": \"document2.pdf\", \"page\": 1}, page_content=\"Conteúdo da página 1 do documento 2\")\n",
    "]\n",
    "\n",
    "# Chame a função para verificar\n",
    "calculate_chunk_ids(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Objective**: Calculates and assigns unique IDs to each document chunk, based on the source (document name) and page.\n",
    "\n",
    "- **last_page_id**: Stores the ID of the last processed page.\n",
    "\n",
    "- **current_chunk_index**: Index of the current chunk on the same page.\n",
    "\n",
    "- **chunk_id**: Unique ID generated for each chunk, combining the document name, page number, and chunk index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_chroma(chunks):\n",
    "    embedding_function = get_embedding_function()\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "    existing_items = db.get(include=[])\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "\n",
    "    new_chunks = [chunk for chunk in chunks_with_ids if chunk.metadata[\"id\"] not in existing_ids]\n",
    "\n",
    "    if new_chunks:\n",
    "        db.add_documents(new_chunks, ids=[chunk.metadata[\"id\"] for chunk in new_chunks])\n",
    "        db.persist()\n",
    "        print(f\"Added {len(new_chunks)} new documents to Chroma.\")\n",
    "    else:\n",
    "        print(\"No new documents to add.\")\n",
    "\n",
    "    # Verificação adicional para garantir que os documentos foram adicionados corretamente\n",
    "    existing_items = db.get(include=[])\n",
    "    print(f\"Existing documents in Chroma: {len(existing_items['ids'])}\")\n",
    "\n",
    "add_to_chroma(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Objective**: Adds chunks to the Chroma database, ensuring that only new chunks are added.\n",
    "\n",
    "- **embedding_function**: Retrieves the custom embedding function.\n",
    "\n",
    "- **db**: Initializes the connection to the Chroma database using the embedding function.\n",
    "\n",
    "- **chunks_with_ids**: Prepares chunks with calculated IDs.\n",
    "\n",
    "- **existing_items**: Retrieves existing documents in Chroma.\n",
    "\n",
    "- **new_chunks**: Filters out new chunks that are not already in Chroma.\n",
    "\n",
    "- **db.add_documents**: Adds the new chunks to the database.\n",
    "\n",
    "- **db.persist**: Persists the changes to the Chroma database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "\n",
    "def get_embedding_function():\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Import**: Imports the `OllamaEmbeddings` class for using text embeddings.\n",
    "\n",
    "- **Function get_embedding_function**: Creates and returns an instance of `OllamaEmbeddings` with the `nomic-embed-text` model.\n",
    "\n",
    "- **Usage**: The function provides an object that can be used to generate vector representations (embeddings) for texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query_text):\n",
    "    print(\"Initializing embedding function...\")  # Debug: Inicializando função de embeddings\n",
    "    embedding_function = get_embedding_function()\n",
    "    print(\"Embedding function initialized.\")  # Debug: Função de embeddings inicializada\n",
    "\n",
    "    print(f\"Loading Chroma database from {CHROMA_PATH}...\")  # Debug: Carregando Chroma DB\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "    print(\"Chroma database loaded.\")  # Debug: Chroma DB carregado\n",
    "\n",
    "    print(f\"Performing similarity search for query: '{query_text}'...\")  # Debug: Realizando busca por similaridade\n",
    "    results = db.similarity_search_with_score(query_text, k=5)\n",
    "    print(f\"Search completed. Number of results: {len(results)}\")  # Debug: Busca concluída\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    print(\"Context extracted from results.\")  # Debug: Contexto extraído dos resultados\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    print(f\"Prompt created: {prompt}\")  # Debug: Exibindo prompt criado\n",
    "\n",
    "    print(\"Invoking the model...\")  # Debug: Invocando o modelo\n",
    "    # Adaptando para usar o Ollama com o servidor local\n",
    "    model = Ollama(model=\"mistral\", base_url=\"http://127.0.0.1:11434\")\n",
    "    response_text = model.invoke(prompt)\n",
    "    print(f\"Model response received: {response_text}\")  # Debug: Resposta do modelo recebida\n",
    "\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)  # Exibindo a resposta formatada\n",
    "    return response_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Initializes** the embedding function using `OllamaEmbeddings`.\n",
    "\n",
    "- **Loads** the Chroma database.\n",
    "\n",
    "- **Performs** a similarity search in the database using the query text.\n",
    "\n",
    "- **Extracts** the context from the search results.\n",
    "\n",
    "- **Creates** a prompt for the model based on the context and the question.\n",
    "\n",
    "- **Invokes** the Ollama model to obtain a response.\n",
    "\n",
    "- **Formats** and prints the response along with the sources of the related documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    query_text = \"Quais são algumas das preocupações éticas associadas ao uso de algoritmos de inteligência artificial, conforme descrito no texto?\"\n",
    "    print(f\"Query text received: {query_text}\")  # Debug: Exibe a query recebida\n",
    "    query_rag(query_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Displays** the received query text for debugging.\n",
    "\n",
    "- **Calls** the `query_rag` function with the query text to process the search and obtain a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
