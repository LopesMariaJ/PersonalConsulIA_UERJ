{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import argparse\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "from langchain_community.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "#from get_embedding_function import get_embedding_function\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "argparse: Facilita a criação de comandos de linha de comando e permite que o código receba parâmetros diretamente via CLI.\n",
    "\n",
    "Chroma: Um banco de dados vetorial para armazenar e recuperar embeddings de texto. Utiliza o diretório persistente CHROMA_PATH.\n",
    "\n",
    "ChatPromptTemplate: Gerencia prompts para modelos de linguagem.\n",
    "\n",
    "Ollama: Interface para usar o modelo de linguagem Ollama, que responde a perguntas com base em prompts.\n",
    "\n",
    "PyPDFDirectoryLoader: Carrega documentos PDF de um diretório.\n",
    "\n",
    "get_embedding_function: Importa uma função personalizada que cria embeddings de texto.\n",
    "\n",
    "RecursiveCharacterTextSplitter: Divide documentos em chunks menores, mantendo uma sobreposição entre eles.\n",
    "\n",
    "Document: Classe de documentos usada para organizar e manipular chunks. [OK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_PATH = 'chroma_directory'\n",
    "DATA_PATH = \"pdfs\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHROMA_PATH: Diretório onde o banco de dados Chroma será armazenado.\n",
    "DATA_PATH: Diretório de onde os PDFs serão carregados.\n",
    "PROMPT_TEMPLATE: Um template de prompt que será usado para solicitar respostas do modelo, fornecendo contexto e uma pergunta. [OK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    print(\"ETAPA 01 \" + str(document_loader))\n",
    "    \n",
    "    # Carrega os documentos do diretório\n",
    "    documents = document_loader.load()\n",
    "    \n",
    "    # Verifica e exibe quais arquivos foram encontrados\n",
    "    if not documents:\n",
    "        print(\"Nenhum arquivo PDF encontrado no diretório especificado.\")\n",
    "    else:\n",
    "        print(f\"ETAPA 02: {len(documents)} arquivos encontrados:\")\n",
    "        for doc in documents:\n",
    "            print(f\" - Arquivo: {doc.metadata['source']}\")\n",
    "    \n",
    "    # Divide os documentos em chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=800,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(\"ETAPA 03: \" + str(chunks))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Chame a função para verificar\n",
    "load_and_split_documents()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objetivo: Carrega todos os documentos PDF do diretório especificado e os divide em chunks.\n",
    "document_loader: Inicializa o carregador de PDFs.\n",
    "documents: Carrega todos os documentos do diretório DATA_PATH.\n",
    "text_splitter: Configura o splitter para dividir documentos em chunks de até 2000 caracteres com uma sobreposição de 800 caracteres (Overlap)\n",
    "chunks: Chunks divididos são retornados. [OK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_ids(chunks):\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Extrai as informações do chunk\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "\n",
    "        # Debug: Exibe o estado atual\n",
    "        print(f\"Iteração {i + 1}:\")\n",
    "        print(f\"  Fonte: {source}\")\n",
    "        print(f\"  Página: {page}\")\n",
    "        print(f\"  ID da Página Atual: {current_page_id}\")\n",
    "        print(f\"  Último ID da Página: {last_page_id}\")\n",
    "\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "\n",
    "        # Calcula o ID do chunk\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "\n",
    "        # Debug: Exibe o ID calculado e atribuído\n",
    "        print(f\"  ID do Chunk Calculado: {chunk_id}\")\n",
    "        print(f\"  Chunk Metadata Atualizada: {chunk.metadata}\")\n",
    "\n",
    "        # Atualiza o último ID da página\n",
    "        last_page_id = current_page_id\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Exemplo de chamada da função para verificação\n",
    "# Suponha que você tenha uma lista de chunks para passar para a função\n",
    "chunks = [\n",
    "    Document(metadata={\"source\": \"document1.pdf\", \"page\": 1}, page_content=\"Conteúdo da página 1\"),\n",
    "    Document(metadata={\"source\": \"document1.pdf\", \"page\": 1}, page_content=\"Conteúdo da página 1 - continuação\"),\n",
    "    Document(metadata={\"source\": \"document1.pdf\", \"page\": 2}, page_content=\"Conteúdo da página 2\"),\n",
    "    Document(metadata={\"source\": \"document2.pdf\", \"page\": 1}, page_content=\"Conteúdo da página 1 do documento 2\")\n",
    "]\n",
    "\n",
    "# Chame a função para verificar\n",
    "calculate_chunk_ids(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objetivo: Calcula e atribui IDs únicos para cada chunk de documento, baseados na origem (nome do documento) e página.\n",
    "last_page_id: Armazena o ID da última página processada.\n",
    "current_chunk_index: Índice do chunk atual na mesma página.\n",
    "chunk_id: ID único gerado para cada chunk, combinando nome do documento, número da página e índice do chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_chroma(chunks):\n",
    "    embedding_function = get_embedding_function()\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "    existing_items = db.get(include=[])\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "\n",
    "    new_chunks = [chunk for chunk in chunks_with_ids if chunk.metadata[\"id\"] not in existing_ids]\n",
    "\n",
    "    if new_chunks:\n",
    "        db.add_documents(new_chunks, ids=[chunk.metadata[\"id\"] for chunk in new_chunks])\n",
    "        db.persist()\n",
    "        print(f\"Added {len(new_chunks)} new documents to Chroma.\")\n",
    "    else:\n",
    "        print(\"No new documents to add.\")\n",
    "\n",
    "    # Verificação adicional para garantir que os documentos foram adicionados corretamente\n",
    "    existing_items = db.get(include=[])\n",
    "    print(f\"Existing documents in Chroma: {len(existing_items['ids'])}\")\n",
    "\n",
    "add_to_chroma(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objetivo: Adiciona chunks ao banco de dados Chroma, garantindo que apenas chunks novos sejam adicionados.\n",
    "embedding_function: Obtém a função de embedding personalizada.\n",
    "db: Inicializa a conexão com o banco de dados Chroma usando a função de embedding.\n",
    "chunks_with_ids: Chunks com IDs calculados são preparados.\n",
    "existing_items: Recupera documentos existentes no Chroma.\n",
    "new_chunks: Filtra chunks novos que não estão no Chroma.\n",
    "db.add_documents: Adiciona os novos chunks ao banco de dados.\n",
    "db.persist: Persiste as mudanças no banco de dados Chroma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
    "\n",
    "def get_embedding_function():\n",
    "    embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação: Importa a classe OllamaEmbeddings para usar embeddings de texto.\n",
    "Função get_embedding_function: Cria e retorna uma instância do OllamaEmbeddings com o modelo nomic-embed-text.\n",
    "\n",
    "Uso: A função fornece um objeto que pode ser usado para gerar representações vetoriais (embeddings) para textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query_text):\n",
    "    print(\"Initializing embedding function...\")  # Debug: Inicializando função de embeddings\n",
    "    embedding_function = get_embedding_function()\n",
    "    print(\"Embedding function initialized.\")  # Debug: Função de embeddings inicializada\n",
    "\n",
    "    print(f\"Loading Chroma database from {CHROMA_PATH}...\")  # Debug: Carregando Chroma DB\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "    print(\"Chroma database loaded.\")  # Debug: Chroma DB carregado\n",
    "\n",
    "    print(f\"Performing similarity search for query: '{query_text}'...\")  # Debug: Realizando busca por similaridade\n",
    "    results = db.similarity_search_with_score(query_text, k=5)\n",
    "    print(f\"Search completed. Number of results: {len(results)}\")  # Debug: Busca concluída\n",
    "\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "    print(\"Context extracted from results.\")  # Debug: Contexto extraído dos resultados\n",
    "\n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    print(f\"Prompt created: {prompt}\")  # Debug: Exibindo prompt criado\n",
    "\n",
    "    print(\"Invoking the model...\")  # Debug: Invocando o modelo\n",
    "    # Adaptando para usar o Ollama com o servidor local\n",
    "    model = Ollama(model=\"mistral\", base_url=\"http://127.0.0.1:11434\")\n",
    "    response_text = model.invoke(prompt)\n",
    "    print(f\"Model response received: {response_text}\")  # Debug: Resposta do modelo recebida\n",
    "\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(formatted_response)  # Exibindo a resposta formatada\n",
    "    return response_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Inicializa a função de embeddings usando `OllamaEmbeddings`.\n",
    "Carrega o banco de dados Chroma.\n",
    "Realiza uma busca por similaridade no banco de dados usando o texto da consulta.\n",
    "Extrai o contexto dos resultados da busca.\n",
    "Cria um prompt para o modelo baseado no contexto e na pergunta.\n",
    "Invoca o modelo Ollama para obter uma resposta.\n",
    "Formata e imprime a resposta e as fontes dos documentos relacionados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    query_text = \"Quais são algumas das preocupações éticas associadas ao uso de algoritmos de inteligência artificial, conforme descrito no texto?\"\n",
    "    print(f\"Query text received: {query_text}\")  # Debug: Exibe a query recebida\n",
    "    query_rag(query_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exibe o texto da consulta recebido, para depuração.\n",
    "Chama a função query_rag com o texto da consulta para processar a busca e obter uma resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
